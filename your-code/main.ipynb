{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T13:32:05.385499Z",
     "start_time": "2024-10-01T13:32:05.356302Z"
    }
   },
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hibi9\\AppData\\Local\\Temp\\ipykernel_23944\\3391910190.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:29:31.437255Z",
     "start_time": "2024-10-01T14:29:31.417189Z"
    }
   },
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.corpus.reader.wordnet as wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hibi9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T13:32:07.280572Z",
     "start_time": "2024-10-01T13:32:06.789114Z"
    }
   },
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T13:32:07.358721Z",
     "start_time": "2024-10-01T13:32:07.340825Z"
    }
   },
   "source": "# I will split them later, after all the cleanup below, to prevent doing this on 2 datas",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T13:32:07.614806Z",
     "start_time": "2024-10-01T13:32:07.579456Z"
    }
   },
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T13:32:10.090707Z",
     "start_time": "2024-10-01T13:32:07.835571Z"
    }
   },
   "source": [
    "def remove_html_elements(text):\n",
    "    # Remove inline CSS\n",
    "    text = re.sub(r'<style.*?</style>', '', text)\n",
    "    text = re.sub(r'(<[^>]+)\\sstyle=\".*?\"', r'\\1', text)\n",
    "\n",
    "    # Remove Javascript\n",
    "    text = re.sub(r'<script.*?</script>', '', text)\n",
    "\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'', '', text)\n",
    "\n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove leading or ending spaces\n",
    "    return text.strip()\n",
    "\n",
    "data['preprocessed_text'] = data['text'].apply(remove_html_elements)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T13:32:11.417824Z",
     "start_time": "2024-10-01T13:32:10.199053Z"
    }
   },
   "source": [
    "def clean_text(text):\n",
    "  # Remove special Characters\n",
    "  text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "  # Remove numbers\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "  # Remove all single caracters\n",
    "  text = re.sub(r'\\b\\w\\b', '', text)\n",
    "\n",
    "  # Remove single caracters at the beginning\n",
    "  text = re.sub(r'^\\w\\s', '', text)\n",
    "\n",
    "  # replace multiple spaces with just 1\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "  # Remove prefix \"b\"\n",
    "  text = re.sub(r'^b\\s', '', text)\n",
    "\n",
    "  # to lowercase\n",
    "  text = text.lower()\n",
    "\n",
    "  return text\n",
    "\n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(clean_text)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T13:32:13.865082Z",
     "start_time": "2024-10-01T13:32:11.503441Z"
    }
   },
   "source": [
    "words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  text = ' '.join([word for word in text.split() if word not in words])\n",
    "  return text\n",
    "\n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(remove_stopwords)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T13:58:44.493614Z",
     "start_time": "2024-10-01T13:58:12.413395Z"
    }
   },
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0]\n",
    "    tag_dict = { \"J\": wordnet.ADJ,\n",
    "               \"V\": wordnet.VERB,\n",
    "               \"N\": wordnet.NOUN,\n",
    "               \"R\": wordnet.ADV }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize(line):\n",
    "    final_doc = []\n",
    "    for word in line.split():\n",
    "        final_doc.append(WordNetLemmatizer().lemmatize(word, pos=get_wordnet_pos(word)))\n",
    "    return final_doc\n",
    "\n",
    "data['lemmatized'] = data['preprocessed_text'].apply(lemmatize)"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T13:59:27.511401Z",
     "start_time": "2024-10-01T13:59:27.464798Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  label  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
       "1                                           Will do.      0   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
       "4                                                fyi      0   \n",
       "\n",
       "                                   preprocessed_text  \\\n",
       "0  dear sir strictly private business proposal mi...   \n",
       "1                                                      \n",
       "2  noracheryl emailed dozens memos haiti weekend ...   \n",
       "3  dear sirfmadamc know proposal might surprise e...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [dear, sir, strictly, private, business, propo...  \n",
       "1                                                 []  \n",
       "2  [noracheryl, email, dozen, memo, haiti, weeken...  \n",
       "3  [dear, sirfmadamc, know, proposal, might, surp...  \n",
       "4                                              [fyi]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "      <td>[dear, sir, strictly, private, business, propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>noracheryl emailed dozens memos haiti weekend ...</td>\n",
       "      <td>[noracheryl, email, dozen, memo, haiti, weeken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sirfmadamc know proposal might surprise e...</td>\n",
       "      <td>[dear, sirfmadamc, know, proposal, might, surp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>[fyi]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:13:37.511752Z",
     "start_time": "2024-10-01T14:13:37.385150Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "ham_texts = data[data['label'] == 0]\n",
    "spam_texts = data[data['label'] == 1]\n",
    "\n",
    "# Zählen der Wörter in Ham- und Spam-Texten\n",
    "ham_word_counts = Counter(\" \".join(ham_texts['preprocessed_text']).split())\n",
    "spam_word_counts = Counter(\" \".join(spam_texts['preprocessed_text']).split())\n",
    "\n",
    "# Top 10 Wörter in Ham und Spam\n",
    "top_10_ham_words = ham_word_counts.most_common(10)\n",
    "top_10_spam_words = spam_word_counts.most_common(10)"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:13:43.283772Z",
     "start_time": "2024-10-01T14:13:43.268453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now I split the data\n",
    "\n",
    "data_train = data[:800]\n",
    "data_val = data[200:]"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:13:52.098533Z",
     "start_time": "2024-10-01T14:13:51.922085Z"
    }
   },
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hibi9\\AppData\\Local\\Temp\\ipykernel_23944\\2855262457.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
      "C:\\Users\\hibi9\\AppData\\Local\\Temp\\ipykernel_23944\\2855262457.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
      "C:\\Users\\hibi9\\AppData\\Local\\Temp\\ipykernel_23944\\2855262457.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
      "C:\\Users\\hibi9\\AppData\\Local\\Temp\\ipykernel_23944\\2855262457.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
      "C:\\Users\\hibi9\\AppData\\Local\\Temp\\ipykernel_23944\\2855262457.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
      "C:\\Users\\hibi9\\AppData\\Local\\Temp\\ipykernel_23944\\2855262457.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text  label  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
       "1                                           Will do.      0   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
       "4                                                fyi      0   \n",
       "\n",
       "                                   preprocessed_text  \\\n",
       "0  dear sir strictly private business proposal mi...   \n",
       "1                                                      \n",
       "2  noracheryl emailed dozens memos haiti weekend ...   \n",
       "3  dear sirfmadamc know proposal might surprise e...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                          lemmatized  money_mark  \\\n",
       "0  [dear, sir, strictly, private, business, propo...           1   \n",
       "1                                                 []           1   \n",
       "2  [noracheryl, email, dozen, memo, haiti, weeken...           1   \n",
       "3  [dear, sirfmadamc, know, proposal, might, surp...           1   \n",
       "4                                              [fyi]           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1      1526  \n",
       "1                 0         0  \n",
       "2                 0       112  \n",
       "3                 1      1392  \n",
       "4                 0         3  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "      <td>[dear, sir, strictly, private, business, propo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>noracheryl emailed dozens memos haiti weekend ...</td>\n",
       "      <td>[noracheryl, email, dozen, memo, haiti, weeken...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sirfmadamc know proposal might surprise e...</td>\n",
       "      <td>[dear, sirfmadamc, know, proposal, might, surp...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>[fyi]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:26:03.479669Z",
     "start_time": "2024-10-01T14:26:02.498982Z"
    }
   },
   "source": [
    "count_vectorizer = CountVectorizer(min_df = 1)\n",
    "\n",
    "cv_ham = count_vectorizer.fit_transform(ham_texts['preprocessed_text'])\n",
    "cv_spam = count_vectorizer.fit_transform(spam_texts['preprocessed_text'])"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:32:06.469737Z",
     "start_time": "2024-10-01T14:32:04.871677Z"
    }
   },
   "source": [
    "tfidfvector = TfidfVectorizer(ngram_range=(2,2))\n",
    "tfidf_ham = tfidfvector.fit_transform(ham_texts['preprocessed_text'])\n",
    "tfidf_spam = tfidfvector.fit_transform(spam_texts['preprocessed_text'])\n",
    "\n",
    "print(tfidf_ham.shape)\n",
    "print(tfidf_spam.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 17343)\n",
      "(442, 47445)\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:37:06.327661Z",
     "start_time": "2024-10-01T14:37:03.862889Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "all_texts = pd.concat([ham_texts['preprocessed_text'], spam_texts['preprocessed_text']], ignore_index=True)\n",
    "all_classes = pd.concat([ham_texts['label'], spam_texts['label']], ignore_index=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorized_data = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, all_classes, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.905\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
